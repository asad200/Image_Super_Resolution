{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import skimage\n",
    "import cv2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.layers import Conv2D, Input\n",
    "from tensorflow.keras import Model\n",
    "from skimage.measure import compare_ssim as ssim\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "# function to calculate the peak signal to noise ratio of low resolution and high resolution\n",
    "def psnr(l_res, h_res):\n",
    "    \n",
    "    # convert the image data to floats\n",
    "    l_resData = l_res.astype(float)\n",
    "    h_resData = h_res.astype(float)\n",
    "    \n",
    "    # calculate the difference\n",
    "    diff = h_resData - l_resData\n",
    "    diff = diff.flatten('C')\n",
    "    \n",
    "    # calculate the root mean square difference\n",
    "    rmsd = math.sqrt(np.mean(diff ** 2.))\n",
    "    \n",
    "    # calculate the psnr\n",
    "    psnr = 20 * math.log10(255. / rmsd)\n",
    "    \n",
    "    return psnr\n",
    "\n",
    "# function for mean squared error\n",
    "def mse(l_res, h_res):\n",
    "    \n",
    "    # sum of squared differences of two images\n",
    "    error = np.sum((l_res.astype(float) - h_res.astype(float)) ** 2)\n",
    "    \n",
    "    # divide by total number of pixels\n",
    "    error /= float(l_res.shape[0] * h_res.shape[1])\n",
    "    return error\n",
    "\n",
    "# compare the qulity of low-res and high-res images\n",
    "def compare_images(l_res, h_res):\n",
    "    \n",
    "    results = []\n",
    "    results.append(psnr(l_res, h_res))\n",
    "    results.append(mse(l_res, h_res))\n",
    "    results.append(ssim(l_res, h_res, multichannel=True))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# degrade images\n",
    "\n",
    "def degrade_images(path, value):\n",
    "    \n",
    "    # for all the files in the given path\n",
    "    for file in os.listdir(path):\n",
    "        \n",
    "        # read the file using cv2\n",
    "        img = cv2.imread(path + '/' + file)\n",
    "        \n",
    "        # find the old and new image dimensions\n",
    "        h, w, c = img.shape\n",
    "        new_h = int(h / value)\n",
    "        new_w = int(h / value)\n",
    "        \n",
    "        # downsize the image\n",
    "        img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        # upsize the image\n",
    "        img = cv2.resize(img, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        # save the image\n",
    "        print('Saving {}'.format(file))\n",
    "        cv2.imwrite('Test_degrded/{}'.format(file), img) \n",
    "        \n",
    "        \n",
    "# image pre processing\n",
    "def size_mod(img, factor):\n",
    "    temp_size = img.shape\n",
    "    size = temp_size[0:2]\n",
    "    size = size - np.mod(size, factor)\n",
    "    return img[0:size[0], 1:size[1]]\n",
    "\n",
    "\n",
    "def crop(img, edge):\n",
    "    return img[edge:-edge, edge:-edge]        \n",
    "        \n",
    "    \n",
    "    \n",
    "def test(test_path, model):\n",
    "    \n",
    "    #model = srcnn_model()\n",
    "    #model.load_weights('srcnn_weights.h5')\n",
    "    \n",
    "    # load high res and and low res images\n",
    "    path, file = os.path.split(test_path)\n",
    "    lr = cv2.imread(test_path)\n",
    "    hr = cv2.imread('org_images/{}'.format(file))\n",
    "    \n",
    "    # take the mode of the images\n",
    "    lr = size_mod(lr, 3)\n",
    "    hr = size_mod(hr, 3)\n",
    "    \n",
    "    # convert the images to YCrCb color space\n",
    "    ycrcb = cv2.cvtColor(lr, cv2.COLOR_BGR2YCrCb)\n",
    "    print(ycrcb.shape)\n",
    "    # extract the Y (luminance) channel from YCrCb space\n",
    "    Y = np.zeros((1, ycrcb.shape[0], ycrcb.shape[1], 1), dtype=float)\n",
    "    Y[0, :, :, 0] = ycrcb[:, :, 0].astype(float) / 255\n",
    "    \n",
    "    # make a prediction using trained model\n",
    "    prediction = model.predict(Y, batch_size=1)\n",
    "    \n",
    "    # post procces the images\n",
    "    prediction *= 255\n",
    "    prediction[prediction > 255] = 255\n",
    "    prediction[prediction < 0] = 0\n",
    "    prediction = prediction.astype(np.uint8)\n",
    "    print(prediction.shape)\n",
    "    # reconstruct the image in BGR space\n",
    "    # note the predicted image lost the 4 pixels on each side therefore we need the crop\n",
    "    # the image with a factor of 6\n",
    "    ycrcb = crop(ycrcb, 4)\n",
    "    ycrcb[:, :, 0] = prediction[0, :, :, 0] \n",
    "    recon_image = cv2.cvtColor(ycrcb, cv2.COLOR_YCrCb2BGR)\n",
    "    \n",
    "    # remove the border of the lr and hr image for comparison\n",
    "    lr = crop(lr.astype(np.uint8), 4)\n",
    "    hr = crop(hr.astype(np.uint8), 4)\n",
    "    \n",
    "    # image comparison\n",
    "    metrics = []\n",
    "    metrics.append(compare_images(lr, hr))\n",
    "    metrics.append(compare_images(recon_image, hr))\n",
    "    \n",
    "    # return hr, lr, reconstructed image and metrics\n",
    "    return hr, lr, recon_image, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image pre processing, split to f*f patches, extract Luminance channel and prepare train data labels\n",
    "\n",
    "PATCH_SIZE = 32\n",
    "STRIDE = 14\n",
    "FACTOR = 2\n",
    "\n",
    "def image_split(path):\n",
    "    \n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    for i, file in enumerate(os.listdir(path)):\n",
    "        \n",
    "        # read the file using cv2\n",
    "        hr = cv2.imread(path + '/' + file)\n",
    "        \n",
    "        # change the image color channel to YCrCb\n",
    "        hr = cv2.cvtColor(hr, cv2.COLOR_BGR2YCrCb)\n",
    "        \n",
    "        # find the old and new image dimensions\n",
    "        h, w, c = hr.shape\n",
    "        \n",
    "        # degrade the images by downsizing and upsizing\n",
    "        new_h = int(h / FACTOR)\n",
    "        new_w = int(h / FACTOR) \n",
    "        lr = cv2.resize(hr, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "        lr = cv2.resize(lr, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        # number of stride steps\n",
    "        w_steps = int((w -(PATCH_SIZE - STRIDE)) / STRIDE)\n",
    "        h_steps = int((h -(PATCH_SIZE - STRIDE)) / STRIDE)\n",
    "        \n",
    "        #print('w: {}'.format(w))\n",
    "        #print('h: {}'.format(h))\n",
    "        #print('w_steps: {}'.format(w_steps))\n",
    "        #print('h_steps: {}'.format(h_steps))\n",
    "        \n",
    "        Y_hr = np.zeros((hr.shape[0], hr.shape[1], 1), dtype=float)\n",
    "        Y_hr[:, :, 0] = hr[:, :, 0].astype(float) / 255\n",
    "        \n",
    "        Y_lr = np.zeros((lr.shape[0], lr.shape[1], 1), dtype=float)\n",
    "        Y_lr[:, :, 0] = lr[:, :, 0].astype(float) / 255\n",
    "        \n",
    "        for i in range(w_steps - 1):\n",
    "            for j in range(h_steps - 1):\n",
    "                \n",
    "                hr_patch = Y_hr[i * STRIDE: i * STRIDE + PATCH_SIZE , j * STRIDE: j * STRIDE + PATCH_SIZE]\n",
    "                lr_patch = Y_lr[i * STRIDE: i * STRIDE + PATCH_SIZE , j * STRIDE: j * STRIDE + PATCH_SIZE]\n",
    "                \n",
    "                if hr_patch.shape[0] == hr_patch.shape[1]:\n",
    "                    x_train.append(hr_patch)\n",
    "                    y_train.append(crop(lr_patch, 4)) \n",
    "    return np.array(x_train, dtype=float), np.array(y_train, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving flowers.bmp\n",
      "Saving baboon.bmp\n",
      "Saving barbara.bmp\n",
      "Saving bridge.bmp\n",
      "Saving coastguard.bmp\n",
      "Saving comic.bmp\n",
      "Saving face.bmp\n",
      "Saving foreman.bmp\n",
      "Saving lenna.bmp\n",
      "Saving man.bmp\n",
      "Saving monarch.bmp\n",
      "Saving pepper.bmp\n",
      "Saving ppt3.bmp\n",
      "Saving zebra.bmp\n",
      "Saving baby_GT.bmp\n",
      "Saving bird_GT.bmp\n",
      "Saving butterfly_GT.bmp\n",
      "Saving head_GT.bmp\n",
      "Saving woman_GT.bmp\n"
     ]
    }
   ],
   "source": [
    "degrade_images('Test/', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asad/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:38: UserWarning: DEPRECATED: skimage.measure.compare_ssim has been moved to skimage.metrics.structural_similarity. It will be removed from skimage.measure in version 0.18.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flowers.bmp\n",
      "PSNR: 26.27259062868604\n",
      "MSE: 460.1956961325967\n",
      "SSIM: 0.8374632814805686\n",
      "\n",
      "baboon.bmp\n",
      "PSNR: 22.13160980892662\n",
      "MSE: 1194.099825\n",
      "SSIM: 0.6322050631319908\n",
      "\n",
      "barbara.bmp\n",
      "PSNR: 24.985291313715283\n",
      "MSE: 618.9741102430555\n",
      "SSIM: 0.7807014586623002\n",
      "\n",
      "bridge.bmp\n",
      "PSNR: 25.850528790115554\n",
      "MSE: 507.1643714904785\n",
      "SSIM: 0.7804245912255268\n",
      "\n",
      "coastguard.bmp\n",
      "PSNR: 27.129127410105276\n",
      "MSE: 377.8234197443182\n",
      "SSIM: 0.7491459914768033\n",
      "\n",
      "comic.bmp\n",
      "PSNR: 25.127913186306913\n",
      "MSE: 598.9772077562327\n",
      "SSIM: 0.8799566225711454\n",
      "\n",
      "face.bmp\n",
      "PSNR: 30.99220650287191\n",
      "MSE: 155.23189718546524\n",
      "SSIM: 0.8008439492289884\n",
      "\n",
      "foreman.bmp\n",
      "PSNR: 29.83350956793885\n",
      "MSE: 202.69855784406565\n",
      "SSIM: 0.9250699266756456\n",
      "\n",
      "lenna.bmp\n",
      "PSNR: 31.47349297867539\n",
      "MSE: 138.94800567626953\n",
      "SSIM: 0.8460989200521499\n",
      "\n",
      "man.bmp\n",
      "PSNR: 27.22646369798821\n",
      "MSE: 369.4496383666992\n",
      "SSIM: 0.8214950645456561\n",
      "\n",
      "monarch.bmp\n",
      "PSNR: 28.69128492283592\n",
      "MSE: 263.6775309244792\n",
      "SSIM: 0.9265469628688131\n",
      "\n",
      "pepper.bmp\n",
      "PSNR: 29.88947161686106\n",
      "MSE: 200.1033935546875\n",
      "SSIM: 0.8357937568464359\n",
      "\n",
      "ppt3.bmp\n",
      "PSNR: 25.678746114337542\n",
      "MSE: 527.6269912167459\n",
      "SSIM: 0.9367078230371364\n",
      "\n",
      "zebra.bmp\n",
      "PSNR: 26.013369776905925\n",
      "MSE: 488.50007855939526\n",
      "SSIM: 0.8451100253708761\n",
      "\n",
      "baby_GT.bmp\n",
      "PSNR: 34.371806409661986\n",
      "MSE: 71.28874588012695\n",
      "SSIM: 0.9356987872724932\n",
      "\n",
      "bird_GT.bmp\n",
      "PSNR: 32.896644728720005\n",
      "MSE: 100.12375819830247\n",
      "SSIM: 0.9533644866026473\n",
      "\n",
      "butterfly_GT.bmp\n",
      "PSNR: 24.782076560337416\n",
      "MSE: 648.6254119873047\n",
      "SSIM: 0.8791344763843051\n",
      "\n",
      "head_GT.bmp\n",
      "PSNR: 31.020502848237534\n",
      "MSE: 154.2237755102041\n",
      "SSIM: 0.8011121330733371\n",
      "\n",
      "woman_GT.bmp\n",
      "PSNR: 30.992323917727237\n",
      "MSE: 155.22770042839656\n",
      "SSIM: 0.9547414789038974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compare the image quality metrics\n",
    "\n",
    "for img in os.listdir('Test_degrded/'):\n",
    "    \n",
    "    lr = cv2.imread('Test_degrded/{}'.format(img))\n",
    "    hr = cv2.imread('Test/{}'.format(img))\n",
    "    \n",
    "    # calculate the metrics\n",
    "    metrics = compare_images(lr, hr)\n",
    "    \n",
    "    # print the results\n",
    "    print('{}\\nPSNR: {}\\nMSE: {}\\nSSIM: {}\\n'.format(img, metrics[0], metrics[1], metrics[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the SRCNN model\n",
    "\n",
    "def srcnn_model():\n",
    "    inputs = tf.keras.Input(shape=(None, None, 1))\n",
    "    x = Conv2D(128, (9, 9), padding='valid', activation='relu',\n",
    "               kernel_initializer='glorot_uniform', use_bias=True)(inputs)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu',\n",
    "               kernel_initializer='glorot_uniform', use_bias=True)(x)\n",
    "    \n",
    "    outputs = Conv2D(1, (5, 5), padding='same', activation='linear',\n",
    "               kernel_initializer='glorot_uniform', use_bias=True)(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='SRCNN')\n",
    "    \n",
    "    # define loss and optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=0.0003)\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SRCNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, None, 1)]   0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, None, None, 128)   10496     \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, None, None, 64)    73792     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, None, None, 1)     1601      \n",
      "=================================================================\n",
      "Total params: 85,889\n",
      "Trainable params: 85,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model = srcnn_model()\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the training \n",
    "x_train, y_train = image_split('Train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size:\n",
      " Training data: (15272, 32, 32, 1)\n",
      " Training Labels: (15272, 24, 24, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Size:\\n Training data: {}\\n Training Labels: {}\\n'.format(x_train.shape, y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15272 samples\n",
      "Epoch 1/2\n",
      " 5344/15272 [=========>....................] - ETA: 3:05 - loss: 0.0057 - mean_squared_error: 0.0057"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-1ddd00c905dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = my_model.fit(x_train, y_train, epochs=2, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Train/'\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "for i, file in enumerate(os.listdir(path)):\n",
    "        \n",
    "        # read the file using cv2\n",
    "        img = cv2.imread(path + '/' + file)\n",
    "        \n",
    "        # find the old and new image dimensions\n",
    "        h, w, c = img.shape\n",
    "        \n",
    "        ycrcb = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)\n",
    "        #print(ycrcb.shape)\n",
    "        # extract the Y (luminance) channel from YCrCb space\n",
    "        Y = np.zeros((ycrcb.shape[0], ycrcb.shape[1], 1), dtype=float)\n",
    "        Y[:, :, 0] = ycrcb[:, :, 0].astype(float) / 255\n",
    "        if i == 0:\n",
    "            Y2 = Y\n",
    "            \n",
    "        print(Y.shape)\n",
    "        data.append(Y2)\n",
    "        ycrcb = crop(ycrcb, 4)\n",
    "        Y = np.zeros((ycrcb.shape[0], ycrcb.shape[1], 1), dtype=float)\n",
    "        Y[:, :, 0] = ycrcb[:, :, 0].astype(float) / 255\n",
    "        if i == 0:\n",
    "            Y3 = Y\n",
    "        labels.append(Y3)\n",
    "        print('{} h: {} w: {} c: {} \\n'.format(file, h, w, c))\n",
    "        \n",
    "data = np.array(data, dtype=float)\n",
    "labels = np.array(labels, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = srcnn_model()\n",
    "\n",
    "model2.fit(data, labels, epochs=10, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = srcnn_model()\n",
    "\n",
    "model2.fit(x_train, y_train, epochs=2, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr, lr, result, metrics = test('degraded_images/flowers.bmp', model2)\n",
    "\n",
    "# compare the quality of images\n",
    "print('Degraded Image: \\nPSNR: {}\\nMSE: {}\\nSSIM: {}\\n'.format(metrics[0][0], metrics[0][1], metrics[0][2]))\n",
    "print('Reconstructed Image: \\nPSNR: {}\\nMSE: {}\\nSSIM: {}\\n'.format(metrics[1][0], metrics[1][1], metrics[1][2]))\n",
    "\n",
    "# display images side by side\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 8))\n",
    "axs[0].imshow(cv2.cvtColor(hr, cv2.COLOR_BGR2RGB))\n",
    "axs[0].set_title('Orignal')\n",
    "axs[1].imshow(cv2.cvtColor(lr, cv2.COLOR_BGR2RGB))\n",
    "axs[1].set_title('Degraded')\n",
    "axs[2].imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\n",
    "axs[2].set_title('Reconstructed')\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    #print(ycrcb.shape)\n",
    "        # extract the Y (luminance) channel from YCrCb space\n",
    "        Y = np.zeros((ycrcb.shape[0], ycrcb.shape[1], 1), dtype=float)\n",
    "        Y[:, :, 0] = ycrcb[:, :, 0].astype(float) / 255\n",
    "        if i == 0:\n",
    "            Y2 = Y\n",
    "            \n",
    "        print(Y.shape)\n",
    "        data.append(Y2)\n",
    "        ycrcb = crop(ycrcb, 4)\n",
    "        Y = np.zeros((ycrcb.shape[0], ycrcb.shape[1], 1), dtype=float)\n",
    "        Y[:, :, 0] = ycrcb[:, :, 0].astype(float) / 255\n",
    "        if i == 0:\n",
    "            Y3 = Y\n",
    "        labels.append(Y3)\n",
    "        print('{} h: {} w: {} c: {} \\n'.format(file, h, w, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr, lr, result, metrics = test('degraded_images/flowers.bmp')\n",
    "\n",
    "# compare the quality of images\n",
    "print('Degraded Image: \\nPSNR: {}\\nMSE: {}\\nSSIM: {}\\n'.format(metrics[0][0], metrics[0][1], metrics[0][2]))\n",
    "print('Reconstructed Image: \\nPSNR: {}\\nMSE: {}\\nSSIM: {}\\n'.format(metrics[1][0], metrics[1][1], metrics[1][2]))\n",
    "\n",
    "# display images side by side\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 8))\n",
    "axs[0].imshow(cv2.cvtColor(hr, cv2.COLOR_BGR2RGB))\n",
    "axs[0].set_title('Orignal')\n",
    "axs[1].imshow(cv2.cvtColor(lr, cv2.COLOR_BGR2RGB))\n",
    "axs[1].set_title('Degraded')\n",
    "axs[2].imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\n",
    "axs[2].set_title('Reconstructed')\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
